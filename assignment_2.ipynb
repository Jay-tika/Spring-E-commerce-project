{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5HqZKkUZatmwAyydWTnrQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TikaAbstract/Spring-E-commerce-project/blob/main/assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_qTgZSh7xB2",
        "outputId": "8d8e2bed-63f2-4b73-eeae-1ab007b30532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: gensim\n",
            "Version: 4.3.2\n",
            "Summary: Python framework for fast Vector Space Modelling\n",
            "Home-page: https://radimrehurek.com/gensim/\n",
            "Author: Radim Rehurek\n",
            "Author-email: me@radimrehurek.com\n",
            "License: LGPL-2.1-only\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, scipy, smart-open\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Load the pre-trained 'word2vec-google-news-300' model\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Read the synonym.csv file and create other 2 files\n",
        "dataset = pd.read_csv('/content/synonym.csv')\n",
        "details_filename = 'word2vec-google-news-300-details.csv'\n",
        "analysis_filename = 'analysis.csv'\n",
        "\n",
        "# Variables to calculate during the analysis\n",
        "total_correct = 0\n",
        "total_answered = 0\n",
        "vocabulary_size = len(model.key_to_index)\n",
        "\n",
        "# Evaluate the dataset and write the details to the (model)details.csv\n",
        "with open(details_filename, mode='w', newline='') as details_file:\n",
        "    details_writer = csv.writer(details_file)\n",
        "\n",
        "    for index, row in dataset.iterrows():\n",
        "        question_word = row['question']\n",
        "        correct_answer = row['answer']\n",
        "        options = [row[str(i)] for i in range(4)]\n",
        "\n",
        "        if question_word in model.key_to_index:\n",
        "            highest_similarity = -1\n",
        "            best_guess = None\n",
        "\n",
        "            for option in options:\n",
        "                if option in model.key_to_index:\n",
        "                    similarity = model.similarity(question_word, option)\n",
        "                    if similarity > highest_similarity:\n",
        "                        highest_similarity = similarity\n",
        "                        best_guess = option\n",
        "\n",
        "            if best_guess:\n",
        "                label = 'correct' if best_guess == correct_answer else 'wrong'\n",
        "                total_answered += 1\n",
        "                total_correct += 1 if label == 'correct' else 0\n",
        "            else:\n",
        "                label = 'guess'\n",
        "\n",
        "            details_writer.writerow([question_word, correct_answer, best_guess if best_guess else 'N/A', label])\n",
        "        else:\n",
        "            details_writer.writerow([question_word, correct_answer, 'N/A', 'guess'])\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = total_correct / total_answered if total_answered > 0 else 0\n",
        "\n",
        "# Write the analysis.csv file\n",
        "with open(analysis_filename, mode='w', newline='') as analysis_file:\n",
        "    analysis_writer = csv.writer(analysis_file)\n",
        "    analysis_writer.writerow(['word2vec-google-news-300', vocabulary_size, total_correct, total_answered, accuracy])\n",
        "\n",
        "# Creating the analysis result to be outputed\n",
        "analysis_result = {\n",
        "    'model_name': 'word2vec-google-news-300',\n",
        "    'vocabulary_size': vocabulary_size,\n",
        "    'correct_labels': total_correct,\n",
        "    'total_answered_without_guessing': total_answered,\n",
        "    'accuracy': accuracy\n",
        "}\n",
        "\n",
        "# Output the analysis result to the user\n",
        "print(analysis_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_8Yh3th5-22",
        "outputId": "81f9f804-8213-4b45-9fd5-45d4856c5333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "{'model_name': 'word2vec-google-news-300', 'vocabulary_size': 3000000, 'correct_labels': 70, 'total_answered_without_guessing': 79, 'accuracy': 0.8860759493670886}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# task_2.3\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_model(model_name, dataset_path, details_filename, analysis_filename):\n",
        "    # Load the pre-trained model\n",
        "    model = api.load(model_name)\n",
        "\n",
        "    # Read the dataset\n",
        "    dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "    # Variables to calculate during the analysis\n",
        "    total_correct = 0\n",
        "    total_answered = 0\n",
        "    vocabulary_size = len(model.key_to_index)\n",
        "\n",
        "    # Evaluate the dataset and write the details\n",
        "    with open(details_filename, mode='w', newline='') as details_file:\n",
        "        details_writer = csv.writer(details_file)\n",
        "\n",
        "        for _, row in dataset.iterrows():\n",
        "            question_word = row['question']\n",
        "            correct_answer = row['answer']\n",
        "            options = [row[str(i)] for i in range(4)]\n",
        "\n",
        "            if question_word in model.key_to_index:\n",
        "                highest_similarity = -1\n",
        "                best_guess = None\n",
        "\n",
        "                for option in options:\n",
        "                    if option in model.key_to_index:\n",
        "                        similarity = model.similarity(question_word, option)\n",
        "                        if similarity > highest_similarity:\n",
        "                            highest_similarity = similarity\n",
        "                            best_guess = option\n",
        "\n",
        "                label = 'correct' if best_guess == correct_answer else 'wrong'\n",
        "                total_answered += 1\n",
        "                total_correct += 1 if label == 'correct' else 0\n",
        "\n",
        "            else:\n",
        "                label = 'guess'\n",
        "\n",
        "            details_writer.writerow([question_word, correct_answer, best_guess if best_guess else 'N/A', label])\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy = total_correct / total_answered if total_answered > 0 else 0\n",
        "\n",
        "    # Append to the analysis.csv file\n",
        "    with open(details_filename, mode='w', newline='') as details_file:\n",
        "      details_writer = csv.writer(details_file)\n",
        "\n",
        "    for _, row in dataset.iterrows():\n",
        "        question_word = row['question']\n",
        "        correct_answer = row['answer']\n",
        "        options = [row[str(i)] for i in range(4)]  # Adjusted to use correct column names\n",
        "\n",
        "# Define your dataset path\n",
        "dataset_path = '/content/synonym.csv'  # Replace with your actual file path\n",
        "\n",
        "# Evaluate for the first model\n",
        "evaluate_model('word2vec-google-news-300', dataset_path, 'word2vec-google-news-300-details.csv', 'analysis.csv')\n",
        "\n",
        "# Evaluate for other models\n",
        "evaluate_model('glove-twitter-100', dataset_path, 'glove-twitter-100-details.csv', 'analysis.csv')\n",
        "evaluate_model('glove-wiki-gigaword-100', dataset_path, 'glove-wiki-gigaword-100-details.csv', 'analysis.csv')\n",
        "\n",
        "evaluate_model('glove-wiki-gigaword-50', dataset_path, 'glove-wiki-gigaword-50-details.csv', 'analysis.csv')\n",
        "evaluate_model('glove-wiki-gigaword-300', dataset_path, 'glove-wiki-gigaword-300-details.csv', 'analysis.csv')\n",
        "\n",
        "# Append to the analysis.csv file in append mode\n",
        "with open(analysis_filename, mode='a', newline='') as analysis_file:\n",
        "    analysis_writer = csv.writer(analysis_file)\n",
        "    analysis_writer.writerow([model_name, vocabulary_size, total_correct, total_answered, accuracy])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1EIBdz5_0S2",
        "outputId": "b3ee9146-07e1-4b56-a9e5-41a94cb4fc66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 387.1/387.1MB downloaded\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 Task2:Comparison with Other Pre-trained Models\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Function to evaluate a model\n",
        "def evaluate_model(model_name, dataset_path, details_filename, analysis_filename):\n",
        "    # Load the model\n",
        "    model = api.load(model_name)\n",
        "\n",
        "    # Read the dataset\n",
        "    dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "    # Initialize variables for analysis\n",
        "    total_correct = 0\n",
        "    total_answered = 0\n",
        "    vocabulary_size = len(model.key_to_index)\n",
        "\n",
        "    # Evaluate the dataset and write the details\n",
        "    with open(details_filename, mode='w', newline='') as details_file:\n",
        "        details_writer = csv.writer(details_file)\n",
        "\n",
        "        for _, row in dataset.iterrows():\n",
        "            question_word = row['question']\n",
        "            correct_answer = row['answer']\n",
        "            options = [row[str(i)] for i in range(4)]\n",
        "\n",
        "            if question_word in model.key_to_index:\n",
        "                highest_similarity = -1\n",
        "                best_guess = None\n",
        "\n",
        "                for option in options:\n",
        "                    if option in model.key_to_index:\n",
        "                        similarity = model.similarity(question_word, option)\n",
        "                        if similarity > highest_similarity:\n",
        "                            highest_similarity = similarity\n",
        "                            best_guess = option\n",
        "\n",
        "                label = 'correct' if best_guess == correct_answer else 'wrong'\n",
        "                total_answered += 1\n",
        "                total_correct += 1 if label == 'correct' else 0\n",
        "            else:\n",
        "                label = 'guess'\n",
        "\n",
        "            details_writer.writerow([question_word, correct_answer, best_guess if best_guess else 'N/A', label])\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy = total_correct / total_answered if total_answered > 0 else 0\n",
        "\n",
        "    # Append to the analysis.csv file\n",
        "    with open(analysis_filename, mode='a', newline='') as analysis_file:\n",
        "        analysis_writer = csv.writer(analysis_file)\n",
        "        analysis_writer.writerow([model_name, vocabulary_size, total_correct, total_answered, accuracy])\n",
        "\n",
        "# Define your dataset path\n",
        "dataset_path = '/content/synonym.csv'\n",
        "\n",
        "# Evaluate for selected models\n",
        "evaluate_model('glove-twitter-100', dataset_path, 'glove-twitter-100-details.csv', 'analysis.csv')\n",
        "evaluate_model('glove-wiki-gigaword-100', dataset_path, 'glove-wiki-gigaword-100-details.csv', 'analysis.csv')\n",
        "\n",
        "evaluate_model('glove-wiki-gigaword-50', dataset_path, 'glove-wiki-gigaword-50-details.csv', 'analysis.csv')\n",
        "evaluate_model('glove-wiki-gigaword-300', dataset_path, 'glove-wiki-gigaword-300-details.csv', 'analysis.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5hstrTkKsL_",
        "outputId": "d1182959-917f-4f75-a1a7-e763eccd8eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    }
  ]
}